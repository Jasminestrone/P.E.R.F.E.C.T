<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>P.E.R.F.E.C.T</title>
    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css">
    <link rel="icon" type="image/x-icon" href="favicon.ico">
    <!-- Add any additional CSS here -->
</head>
<body>
    <div class="container">
        <h1 class="mt-4 text-center">P.E.R.F.E.C.T</h1>
        
        <!-- Voice Input Section -->
        <button id="startButton" class="btn btn-primary mt-3">Start Voice Input</button>
        <div id="output" class="mt-3"></div>

        <!-- Pitch Detection Section -->
        <div class="mt-4">
            <h3>Pitch Detection</h3>
            <button id="resume-button" class="btn btn-secondary">Resume Audio Context</button>
            <div>
                <strong>Pitch:</strong> <span id="pitch">-- Hz</span>
            </div>
            <div>
                <strong>Clarity:</strong> <span id="clarity">-- %</span>
            </div>
        </div>

        <!-- Body Tracking Section -->
        <div class="mt-5">
            <canvas id="canvas" width="640" height="480" style="border: 1px solid black;"></canvas>
        </div>
    </div>

    <!-- TensorFlow.js and Pose Detection -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection"></script>

    <!-- Pitch Detection Library -->
    <script type="module">
        import { PitchDetector } from "https://esm.sh/pitchy@4";

        // Pitch Detection Code
        function updatePitch(analyserNode, detector, input, sampleRate) {
            analyserNode.getFloatTimeDomainData(input);
            const [pitch, clarity] = detector.findPitch(input, sampleRate);

            document.getElementById("pitch").textContent = pitch
                ? `${(Math.round(pitch * 10) / 10).toFixed(1)} Hz`
                : '-- Hz';
            document.getElementById("clarity").textContent = clarity
                ? `${Math.round(clarity * 100)} %`
                : '-- %';
            window.setTimeout(
                () => updatePitch(analyserNode, detector, input, sampleRate),
                100,
            );
        }

        document.addEventListener("DOMContentLoaded", () => {
            const audioContext = new (window.AudioContext || window.webkitAudioContext)();
            const analyserNode = audioContext.createAnalyser();

            document
                .getElementById("resume-button")
                .addEventListener("click", () => audioContext.resume());

            navigator.mediaDevices.getUserMedia({ audio: true }).then((stream) => {
                audioContext.createMediaStreamSource(stream).connect(analyserNode);
                const detector = PitchDetector.forFloat32Array(analyserNode.fftSize);
                const input = new Float32Array(detector.inputLength);
                updatePitch(analyserNode, detector, input, audioContext.sampleRate);
            });
        });
    </script>

    <!-- Speech Recognition and Body Tracking Scripts -->
    <script>
        // Speech Recognition Code
        const startButton = document.getElementById('startButton');
        const outputDiv = document.getElementById('output');

        const grammar = "uh";
        const gram2 = "um";
        const gram3 = "ah";
        const gram4 = "hmm";

        const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        const speechRecognitionList = new (window.SpeechGrammarList || window.webkitSpeechGrammarList)();
        recognition.lang = 'en-US';
        speechRecognitionList.addFromString(grammar, 1);
        speechRecognitionList.addFromString(gram2, 1);
        speechRecognitionList.addFromString(gram3, 1);
        speechRecognitionList.addFromString(gram4, 1);
        recognition.grammars = speechRecognitionList;

        recognition.onstart = () => {
            startButton.textContent = 'Listening...';
        };

        recognition.onresult = (event) => {
            const transcript = event.results[0][0].transcript;
            outputDiv.textContent = `You said: ${transcript}`;
        };

        recognition.onend = () => {
            startButton.textContent = 'Start Voice Input';
        };

        startButton.addEventListener('click', () => {
            recognition.start();
        });

        // Body Tracking Code
        async function initializeBodyTracking() {
            try {
                await tf.setBackend('webgl');
                await tf.ready();

                const detectorConfig = {
                    modelType: poseDetection.movenet.modelType.SINGLEPOSE_LIGHTNING,
                };

                const detector = await poseDetection.createDetector(
                    poseDetection.SupportedModels.MoveNet,
                    detectorConfig
                );

                const canvas = document.getElementById('canvas');
                const ctx = canvas.getContext('2d');
                const video = await setupCamera();

                trackBody(video, detector, canvas, ctx);
            } catch (error) {
                console.error("Error initializing body tracker:", error);
            }
        }

        async function setupCamera() {
            const video = document.createElement('video');
            video.width = 640;
            video.height = 480;
            video.style.display = 'none';
            document.body.appendChild(video);

            const stream = await navigator.mediaDevices.getUserMedia({
                video: { width: 640, height: 480 },
                audio: false,
            });

            video.srcObject = stream;
            await new Promise((resolve) => (video.onloadedmetadata = resolve));
            video.play();
            return video;
        }

        async function trackBody(video, detector, canvas, ctx) {
            const keypointConnections = [
                [0, 1],  // Nose to Left Eye
                [0, 2],  // Nose to Right Eye
                [1, 3],  // Left Eye to Left Ear
                [2, 4],  // Right Eye to Right Ear
                [5, 6],  // Left Shoulder to Right Shoulder
                [5, 7],  // Left Shoulder to Left Elbow
                [7, 9],  // Left Elbow to Left Wrist
                [6, 8],  // Right Shoulder to Right Elbow
                [8, 10], // Right Elbow to Right Wrist
                [5, 11], // Left Shoulder to Left Hip
                [6, 12], // Right Shoulder to Right Hip
                [11, 12],// Left Hip to Right Hip
                [11, 13],// Left Hip to Left Knee
                [13, 15],// Left Knee to Left Ankle
                [12, 14],// Right Hip to Right Knee
                [14, 16] // Right Knee to Right Ankle
            ];

            async function renderFrame() {
                ctx.clearRect(0, 0, canvas.width, canvas.height);
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

                const poses = await detector.estimatePoses(video);
                if (poses && poses.length > 0) {
                    const keypoints = poses[0].keypoints;

                    // Draw keypoints
                    keypoints.forEach((keypoint) => {
                        if (keypoint.score > 0.5) {
                            ctx.beginPath();
                            ctx.arc(keypoint.x, keypoint.y, 5, 0, 2 * Math.PI);
                            ctx.fillStyle = "red";
                            ctx.fill();
                        }
                    });

                    // Draw skeleton
                    keypointConnections.forEach(([startIdx, endIdx]) => {
                        const startPoint = keypoints[startIdx];
                        const endPoint = keypoints[endIdx];
                        if (startPoint.score > 0.5 && endPoint.score > 0.5) {
                            ctx.beginPath();
                            ctx.moveTo(startPoint.x, startPoint.y);
                            ctx.lineTo(endPoint.x, endPoint.y);
                            ctx.strokeStyle = "blue";
                            ctx.lineWidth = 2;
                            ctx.stroke();
                        }
                    });
                }

                requestAnimationFrame(renderFrame);
            }

            renderFrame();
        }

        initializeBodyTracking();
    </script>
</body>
</html>
